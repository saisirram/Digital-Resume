<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Sai Sriram Vundavalli</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="./styles/styles.css">
    </head>
    <body>
        <div id="container--main">
            <a href="index.html">&#x2190; Go Back</a>
            <h1>Quora Question Pairs Similarity Detection</h1>
            <div class="card--techstack"><strong>Technologies:  Python, Numpy, Pandas, Matplotlib, Sklearn, NLTK, FuzzyWuzzy, XGBoost,
                Gensim Word2Vec.</strong></div>
            <p><strong>Project Description: </strong></p>
            <p>Quora is a question-answering platform where distinct users ask questions and also
                answer to other questions. According to a recent study, there are more or less 2.1 million
                people who use Quora every day, and close to 88 Million questions were asked on
                Quora. Random Forest strategy is currently used by Quora to pinpoint duplicate
                questions. The goal of this project is to apply advanced techniques and evaluation
                metrics to evaluate whether the provided pairs of questions have the same meaning
                (which has the same intent). Once successfully identifying duplicate questions, users
                can easily find high-quality answers without spending more time looking for the best
                answer among multiple similar questions, and also less time for writers to answer the
                same questions multiple times.</p>
                <p><strong>Responsibilities: </strong></p>
            <ul>
                <li>Resolved imbalances in the dataset and then it adds new numeric features which are directly related to the textual data.</li>
                <li>Tokenized the textual data and then preprocessed the tokenized data in order to have only the relevant tokens.</li>
                <li>Vectorized the textual tokens using Gensim Word2Vec model so that the machine can easily understand the textual data.</li>
                <li>Applied various Machine Learning models (Logistic Regression, SVM,and XGBoost) and evaluation metrics to fine tune and observe the performance of the dataset across various ML Models.</li>
            </ul>
        </div>
    </body>
</html> 
